<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />




  <meta name="description"
    content="Language-Augmented Visual Action Chunking with Transformers for Bimanual Robotic Manipulation">
  <meta property="og:title" content="LAV-ACT" />
  <meta property="og:description"
    content="Language-Augmented Visual Action Chunking with Transformers for Bimanual Robotic Manipulation" />
  <meta property="og:url" content="https://dktpt44.github.io/LAV-ACT/" />
  <!-- Banner image path should be in the directory specified below. Optimal dimensions: 1200x630 -->
  <meta property="og:image" content="static/images/act_met_fin" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="LAV-ACT  Title">
  <meta name="twitter:description"
    content="Language-Augmented Visual Action Chunking with Transformers for Bimanual Robotic Manipulation">
  <!-- Banner image path should be in the directory specified below. Optimal dimensions: 1200x600 -->
  <meta name="twitter:image" content="static/images/act_met_fin">
  <meta name="twitter:card" content="static/images/act_met_fin">
  <!-- Keywords for indexing LAV-ACT -->

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Keywords related to LAV-ACT">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LAV-ACT</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              LAV-ACT: Language-Augmented Visual Action Chunking with Transformers for Bimanual Robotic Manipulation
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://dktpt44.github.io/" target="_blank"> Dhurba Tripathi</a><sup>1 * </sup></span>

              <span class="author-block">
                Chunting Liu<sup>1 * </sup></span>

              <span class="author-block">

                <a href="https://nirajpudasaini.com.np/" target="_blank">Niraj Pudasaini</a><sup>1 </sup></span>
              <span class="author-block">
                <a href="https://haoyu6427.github.io/yuhao.github.io/" target="_blank">Yu Hao</a><sup>1 </sup></span>
              <span class="author-block">
                Anthony Tzes<sup>1 </sup></span>
              <span class="author-block">
                Yi Fang<sup>1 †</sup></span>

              <!-- <span class="author-block">
                 Anonymous submission.</a><sup>1 †</sup></span> -->


            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup> New York University Abu Dhabi<br>

              </span>
              <span class="eql-cntrb"><small><br><sup>*</sup> Equal contribution</small></span>
              <span class="eql-cntrb"><small><br><sup>†</sup> Corresponding Author</small></span><br>

              <p><a href="https://air.nyuad.nyu.edu/" target="_blank">Embodied AI and Robotics (AIR) Lab</a>, New York
                University Abu Dhabi, UAE</p>

              <span class="author-block"
                style="background: linear-gradient(to right, rgb(255, 0, 0), rgb(0, 255, 0), rgb(0, 0, 0)); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;"><b>In
                  the Proceedings of the
                  11th International Conference on Automation, Robotics, and Applications (ICARA 2025)</b></span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">

                <span class="link-block">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10977578" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/ChunTing5503/act_voltron_film" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                

                <span class="link-block">
                  <a href="https://youtu.be/-xuqGitxpa8" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Presentation</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/act_met_fin.png" alt="MY ALT TEXT" , width="700" />
        <h4 class="subtitle has-text-centered">The overview of the proposed method.</h4>
      </div>





    </div>
  </section>
  <!-- End teaser -->

  <section class="hero teaser">
    <div class="container is-max-desktop">

      <div class="hero-body">
        <div class="successratesholder">
          <img src="static/images/s1.PNG" alt="MY ALT TEXT" , width="100%" />
          <img src="static/images/s2.PNG" alt="MY ALT TEXT" , width="100%" />
        </div>

        <h4 class="subtitle has-text-centered">Success rates for LAV-ACT.</h4>
      </div>




    </div>
  </section>



  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths takeMoresid">


          <h2 class="title is-2"
            style="background: linear-gradient(to right,  indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;">
            Policy rollouts</h2>

          <div class="vid_cont">
            <video id="vid_map1" autoplay controls muted loop height="100%">\
              <source src="static/videos/bi_without.mp4" type="video/mp4">
            </video>
          </div>
          <p>
            <span class="author-block"
              style="background: linear-gradient(to right, rgb(255, 0, 0), rgb(0, 255, 0), rgb(0, 0, 0)); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;"><b>
                Bimanual Pouring without Temporal Aggregation
              </b></span>
          </p>
          <br />
          <br />


          <!-- 

            <p>

              <span class="author-block"
              style="background: linear-gradient(to right, rgb(255, 0, 0), rgb(0, 255, 0), rgb(0, 0, 0)); -webkit-background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;"><b>
                Bimanual Pouring with Temporal Aggregation
              </b></span>
            </p> -->



          <!-- <br /> -->

          <div class="vid_cont">
            <video id="vid_map3" autoplay controls muted loop height="100%">\
              <source src="static/videos/single_without.mp4" type="video/mp4">
            </video>
          </div>
          <p>
            <span class="author-block"
              style="background: linear-gradient(to right, rgb(255, 0, 0), rgb(0, 255, 0), rgb(0, 0, 0)); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;"><b>
                Single Arm Pouring without Temporal Aggregation
              </b></span>
          </p>
          <br />
          <br />


          <div class="vid_cont">
            <video id="vid_map4" autoplay controls muted loop height="100%">\
              <source src="static/videos/single_with.mp4" type="video/mp4">
            </video>
          </div>
          <p>
            <span class="author-block"
              style="background: linear-gradient(to right, rgb(255, 0, 0), rgb(0, 255, 0), rgb(0, 0, 0)); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;"><b>
                Single Arm Pouring with Temporal Aggregation
              </b></span>
          </p>



        </div>
      </div>

    </div>
  </section>

  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-2"
            style="background: linear-gradient(to right,  indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;">
            Abstract</h2>

          <p>
            Bimanual robotic manipulation, involving the coordinated
            use of two robotic arms, is essential for tasks requiring
            complex, synchronous actions. Action Chunking with Transformers
            (ACT) is a representative framework that enables robots to
            break down complex tasks into manageable sequences, facilitating
            autonomous learning of multi-step actions. However, we observe
            critical limitations in the ACT framework: it relies solely on
            visual observations as input, focusing on task-specific action
            predictions, and it uses a simple ResNet-based feature extractor
            for image processing, which is often insufficient for complex
            and multi-view bimanual arm observations. In this paper, we
            introduce an enhanced language-driven version of ACT that
            leverages Voltron—a language-driven representation model—to
            incorporate both visual observations and language prompts into
            dense, multi-modal embeddings. These embeddings are used to
            condition the ResNet backbone feature maps through Featurewise
            Linear Modulation (FiLM), allowing our model to integrate
            contextually relevant linguistic information with visual data for
            more adaptive action chunking. Extensive experiments show that
            our approach significantly improves the performance of bimanual
            robot arms in executing complex, multi-step tasks guided by
            language cues, outperforming traditional ACT methods.
          </p>

          <br />

          <hr />
          <h2 class="title is-2"
            style="background: linear-gradient(to right,  indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;">
            What has Changed?</h2>
          <h5 class="title is-5" style="text-align: center;">
            We present the following technical contributions to enhance
            the ACT, and demonstrate in real-world and simulated environments,
            that our method improves upon the ACT:
          </h5>

          <div class="columns is-centered">
            <!-- reenact. -->
            <div class="column">
              <div class="content">
                <h4 class="title is-5" style="text-align: center;">1. Language-Driven Dense Vector Encoding</h4>
                <p>
                  We introduce a language-driven system to bimanual
                  robotic arm manipulation by integrating the Voltron
                  vision-language encoder.
                </p>
              </div>
            </div>
            <!--/ reenact -->
            <!-- swap. -->
            <div class="column">
              <h4 class="title is-5" style="text-align: center;">2. Feature Map Conditioning with FiLM</h4>
              <div class="columns is-centered">
                <div class="column content">
                  <p>
                    We propose integrating Voltron to condition the ResNet
                    backbone feature maps through Feature-wise Linear
                    Modulation (FiLM), enabling the model to combine linguistic
                    context with visual data for more adaptive action
                    chunking.
                  </p>
                </div>
              </div>
            </div>
            <!--/ swap. -->
          </div>



          <div class="hero-body">
            <img src="static/images/what_changed.png" alt="MY ALT TEXT" , width="1000" />
            <h4 class="subtitle has-text-centered"><b></b></h4>
            Feature extractor pipeline
          </div>

          <br />

          <hr />

          <h2 class="title is-2"
            style="background: linear-gradient(to right,  indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;">
            Real world tasks tested on </h2>

          <div class="hero-body">
            <img src="static/images/task_def.PNG" alt="MY ALT TEXT" , width="1200" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-2"
            style="background: linear-gradient(to right,  indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;">
            Data collection</h2>
          <h5 class="title is-5" style="text-align: center;">
            We collected data using human expert
            demonstrations where operators manually control the robot to
            perform a specific task in the first stage, which is then played
            while recording from the cameras.
          </h5>


          <div class="vid_cont">
            <video id="vid_map" autoplay controls muted loop height="100%">\
              <source src="static/videos/data_collection_process.mp4" type="video/mp4">
            </video>
          </div>

          <br />
          <br />


          <h2 class="title is-3">Data Samples</h2>
          <p>Find all the trainig data in this link: (coming soon)</p>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-video1">
              <video poster="" id="video1" autoplay controls muted loop height="100%">
                <source src="static/videos/v0.mp4" type="video/mp4">
              </video>
            </div>
            <div class="item item-video2">
              <video poster="" id="video2" autoplay controls muted loop height="100%">
                <source src="static/videos/v1.mp4" type="video/mp4">
              </video>
            </div>
          </div>




        </div>
      </div>
    </div>
  </section>



  <!-- Video carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">

      </div>
    </div>
  </section>
  <!-- End video carousel -->






  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550"></iframe>
      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section hero" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTex</h2>
      <pre><code>
          @INPROCEEDINGS{10977578,
              author={Tripathi, Dhurba and Liu, Chunting and Pudasaini, Niraj and Hao, Yu and Tzes, Anthony and Fang, Yi},
              booktitle={2025 11th International Conference on Automation, Robotics, and Applications (ICARA)}, 
              title={LAV-ACT: Language-Augmented Visual Action Chunking with Transformers for Bimanual Robotic Manipulation}, 
              year={2025},
              volume={},
              number={},
              pages={18-22},
              keywords={Visualization;Adaptation models;Robot kinematics;Modulation;Linguistics;Transformers;Manipulators;Feature extraction;Data models;Context modeling;Imitation Learning;Bimanual Manipulation;Behavior cloning},
              doi={10.1109/ICARA64554.2025.10977578}}

      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.

              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>